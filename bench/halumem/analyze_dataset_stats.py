"""
HaluMem Dataset Statistics Analyzer

ç»Ÿè®¡ HaluMem æ•°æ®é›†çš„å„é¡¹æŒ‡æ ‡ï¼š
- æ¯ä¸ªç”¨æˆ·çš„ session æ•°é‡
- æ¯ä¸ª session çš„å¯¹è¯æ•°é‡
- æ¯ä¸ª session çš„å¯¹è¯æ€»é•¿åº¦

Usage:
    python bench/halumem/analyze_dataset_stats.py --data_path /path/to/HaluMem-Medium.jsonl
"""

import json
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import numpy as np
from loguru import logger


@dataclass
class UserStats:
    """å•ä¸ªç”¨æˆ·çš„ç»Ÿè®¡æ•°æ®"""
    user_name: str
    uuid: str
    num_sessions: int
    dialogues_per_session: list[int]  # æ¯ä¸ª session çš„å¯¹è¯æ•°é‡
    dialogue_lengths_per_session: list[int]  # æ¯ä¸ª session çš„å¯¹è¯æ€»é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
    num_chunks_after_split: int  # æŒ‰ 5000 å­—ç¬¦åˆ†å‰²åçš„ chunk æ•°é‡


@dataclass
class DatasetStats:
    """æ•´ä½“æ•°æ®é›†ç»Ÿè®¡"""
    total_users: int
    total_sessions: int
    total_dialogues: int
    
    avg_sessions_per_user: float
    avg_dialogues_per_session: float
    avg_dialogue_length_per_session: float
    
    # è¯¦ç»†åˆ†å¸ƒ
    sessions_per_user_list: list[int]
    dialogues_per_session_list: list[int]
    dialogue_lengths_per_session_list: list[int]
    
    # Content ç»Ÿè®¡
    total_contents: int  # æ‰€æœ‰å¯¹è¯å›åˆçš„ content æ€»æ•°
    content_sizes: list[int]  # æ¯ä¸ª content çš„å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
    min_content_size: int
    max_content_size: int
    percentiles: dict[str, float]  # åˆ†ä½ç‚¹ç»Ÿè®¡ï¼ˆå…¨éƒ¨ï¼‰
    
    # æŒ‰ role åˆ†ç±»çš„ Content ç»Ÿè®¡
    total_user_contents: int
    total_assistant_contents: int
    user_percentiles: dict[str, float]  # user è§’è‰²çš„åˆ†ä½ç‚¹
    assistant_percentiles: dict[str, float]  # assistant è§’è‰²çš„åˆ†ä½ç‚¹
    
    # Session åˆ†å‰²ç»Ÿè®¡
    total_chunks_after_split: int  # æŒ‰ 5000 å­—ç¬¦åˆ†å‰²åçš„æ€» chunk æ•°
    chunks_per_user_list: list[int]  # æ¯ä¸ªç”¨æˆ·åˆ†å‰²åçš„ chunk æ•°é‡
    avg_chunks_per_user: float  # å¹³å‡æ¯ä¸ªç”¨æˆ·çš„ chunk æ•°é‡


class DatasetAnalyzer:
    """æ•°æ®é›†åˆ†æå™¨"""
    
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.user_stats_list: list[UserStats] = []
        self.all_content_sizes: list[int] = []  # æ”¶é›†æ‰€æœ‰ content çš„å¤§å°
        self.user_content_sizes: list[int] = []  # user è§’è‰²çš„ content å¤§å°
        self.assistant_content_sizes: list[int] = []  # assistant è§’è‰²çš„ content å¤§å°
    
    @staticmethod
    def extract_user_name(persona_info: str) -> str:
        """ä» persona_info ä¸­æå–ç”¨æˆ·å"""
        match = re.search(r"Name:\s*(.*?);", persona_info)
        if not match:
            return "Unknown"
        return match.group(1).strip()
    
    @staticmethod
    def calculate_dialogue_length(dialogue: list[dict]) -> int:
        """è®¡ç®—å¯¹è¯çš„æ€»é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰"""
        total_length = 0
        for turn in dialogue:
            content = turn.get("content", "")
            total_length += len(content)
        return total_length
    
    @staticmethod
    def split_session_into_chunks(dialogue: list[dict], max_length: int = 5000) -> int:
        """
        å°†ä¸€ä¸ª session æŒ‰ç…§ max_length åˆ†å‰²æˆå¤šä¸ª chunksã€‚
        è§„åˆ™ï¼š
        1. æ¯æ¬¡æ·»åŠ  2 ä¸ªå¯¹è¯å›åˆï¼ˆuser-assistant å¯¹ï¼‰
        2. å¦‚æœæ·»åŠ åè¶…è¿‡ max_lengthï¼Œå°±å¼€å§‹æ–°çš„ chunk
        3. ä½†æ˜¯æ¯ä¸ª chunk è‡³å°‘åŒ…å« 2 ä¸ªå¯¹è¯å›åˆ
        
        è¿”å›åˆ†å‰²åçš„ chunk æ•°é‡
        """
        if not dialogue:
            return 0
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        # æ¯æ¬¡å¤„ç† 2 ä¸ªå¯¹è¯å›åˆ
        i = 0
        while i < len(dialogue):
            # å– 2 ä¸ªå¯¹è¯å›åˆï¼ˆå¦‚æœä¸è¶³ 2 ä¸ªï¼Œå–å‰©ä½™çš„ï¼‰
            pair = dialogue[i:i+2]
            pair_length = sum(len(turn.get("content", "")) for turn in pair)
            
            # å¦‚æœå½“å‰ chunk ä¸ºç©ºï¼Œç›´æ¥æ·»åŠ ï¼ˆä¿è¯è‡³å°‘ 2 ä¸ªï¼‰
            if not current_chunk:
                current_chunk.extend(pair)
                current_length += pair_length
                i += len(pair)
            else:
                # å¦‚æœæ·»åŠ è¿™ä¸€å¯¹åä¼šè¶…è¿‡é™åˆ¶
                if current_length + pair_length > max_length:
                    # ä¿å­˜å½“å‰ chunkï¼Œå¼€å§‹æ–°çš„ chunk
                    chunks.append(current_chunk)
                    current_chunk = pair
                    current_length = pair_length
                    i += len(pair)
                else:
                    # å¦åˆ™æ·»åŠ åˆ°å½“å‰ chunk
                    current_chunk.extend(pair)
                    current_length += pair_length
                    i += len(pair)
        
        # æ·»åŠ æœ€åä¸€ä¸ª chunk
        if current_chunk:
            chunks.append(current_chunk)
        
        return len(chunks)
    
    def load_and_analyze(self):
        """åŠ è½½å¹¶åˆ†ææ•°æ®é›†"""
        logger.info(f"Loading data from: {self.data_path}")
        
        with open(self.data_path, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue
                
                try:
                    user_data = json.loads(line)
                    self._analyze_user(user_data)
                except json.JSONDecodeError as e:
                    logger.error(f"Error parsing line {line_num}: {e}")
                    continue
        
        logger.info(f"Analyzed {len(self.user_stats_list)} users")
    
    def _analyze_user(self, user_data: dict):
        """åˆ†æå•ä¸ªç”¨æˆ·çš„æ•°æ®"""
        user_name = self.extract_user_name(user_data.get("persona_info", ""))
        uuid = user_data.get("uuid", "")
        sessions = user_data.get("sessions", [])
        
        dialogues_per_session = []
        dialogue_lengths_per_session = []
        total_chunks = 0
        
        for session in sessions:
            dialogue = session.get("dialogue", [])
            num_dialogues = len(dialogue)
            dialogue_length = self.calculate_dialogue_length(dialogue)
            
            dialogues_per_session.append(num_dialogues)
            dialogue_lengths_per_session.append(dialogue_length)
            
            # è®¡ç®—è¿™ä¸ª session åˆ†å‰²åçš„ chunk æ•°é‡
            num_chunks = self.split_session_into_chunks(dialogue, max_length=5000)
            total_chunks += num_chunks
            
            # æ”¶é›†æ¯ä¸ª content çš„å¤§å°ï¼Œå¹¶æŒ‰ role åˆ†ç±»
            for turn in dialogue:
                content = turn.get("content", "")
                content_size = len(content)
                role = turn.get("role", "")
                
                self.all_content_sizes.append(content_size)
                
                if role == "user":
                    self.user_content_sizes.append(content_size)
                elif role == "assistant":
                    self.assistant_content_sizes.append(content_size)
        
        user_stats = UserStats(
            user_name=user_name,
            uuid=uuid,
            num_sessions=len(sessions),
            dialogues_per_session=dialogues_per_session,
            dialogue_lengths_per_session=dialogue_lengths_per_session,
            num_chunks_after_split=total_chunks
        )
        
        self.user_stats_list.append(user_stats)
    
    def compute_dataset_stats(self) -> DatasetStats:
        """è®¡ç®—æ•´ä½“æ•°æ®é›†ç»Ÿè®¡"""
        total_users = len(self.user_stats_list)
        
        sessions_per_user_list = [u.num_sessions for u in self.user_stats_list]
        total_sessions = sum(sessions_per_user_list)
        
        dialogues_per_session_list = []
        dialogue_lengths_per_session_list = []
        
        for user in self.user_stats_list:
            dialogues_per_session_list.extend(user.dialogues_per_session)
            dialogue_lengths_per_session_list.extend(user.dialogue_lengths_per_session)
        
        total_dialogues = sum(dialogues_per_session_list)
        
        # è®¡ç®—å¹³å‡å€¼
        avg_sessions_per_user = total_sessions / total_users if total_users > 0 else 0
        avg_dialogues_per_session = (
            total_dialogues / total_sessions if total_sessions > 0 else 0
        )
        avg_dialogue_length_per_session = (
            sum(dialogue_lengths_per_session_list) / len(dialogue_lengths_per_session_list)
            if dialogue_lengths_per_session_list else 0
        )
        
        # Content ç»Ÿè®¡
        total_contents = len(self.all_content_sizes)
        min_content_size = min(self.all_content_sizes) if self.all_content_sizes else 0
        max_content_size = max(self.all_content_sizes) if self.all_content_sizes else 0
        
        # è®¡ç®—åˆ†ä½ç‚¹ (10%, 15%, 20%, ..., 95%)
        percentile_points = list(range(10, 100, 5))  # 10, 15, 20, ..., 95
        
        # å…¨éƒ¨ content çš„åˆ†ä½ç‚¹
        percentiles = {}
        if self.all_content_sizes:
            content_array = np.array(self.all_content_sizes)
            for p in percentile_points:
                percentiles[f"p{p}"] = float(np.percentile(content_array, p))
        
        # user è§’è‰²çš„åˆ†ä½ç‚¹
        user_percentiles = {}
        if self.user_content_sizes:
            user_array = np.array(self.user_content_sizes)
            for p in percentile_points:
                user_percentiles[f"p{p}"] = float(np.percentile(user_array, p))
        
        # assistant è§’è‰²çš„åˆ†ä½ç‚¹
        assistant_percentiles = {}
        if self.assistant_content_sizes:
            assistant_array = np.array(self.assistant_content_sizes)
            for p in percentile_points:
                assistant_percentiles[f"p{p}"] = float(np.percentile(assistant_array, p))
        
        # Session åˆ†å‰²ç»Ÿè®¡
        chunks_per_user_list = [u.num_chunks_after_split for u in self.user_stats_list]
        total_chunks_after_split = sum(chunks_per_user_list)
        avg_chunks_per_user = (
            total_chunks_after_split / total_users if total_users > 0 else 0
        )
        
        return DatasetStats(
            total_users=total_users,
            total_sessions=total_sessions,
            total_dialogues=total_dialogues,
            avg_sessions_per_user=avg_sessions_per_user,
            avg_dialogues_per_session=avg_dialogues_per_session,
            avg_dialogue_length_per_session=avg_dialogue_length_per_session,
            sessions_per_user_list=sessions_per_user_list,
            dialogues_per_session_list=dialogues_per_session_list,
            dialogue_lengths_per_session_list=dialogue_lengths_per_session_list,
            total_contents=total_contents,
            content_sizes=self.all_content_sizes,
            min_content_size=min_content_size,
            max_content_size=max_content_size,
            percentiles=percentiles,
            total_user_contents=len(self.user_content_sizes),
            total_assistant_contents=len(self.assistant_content_sizes),
            user_percentiles=user_percentiles,
            assistant_percentiles=assistant_percentiles,
            total_chunks_after_split=total_chunks_after_split,
            chunks_per_user_list=chunks_per_user_list,
            avg_chunks_per_user=avg_chunks_per_user
        )
    
    @staticmethod
    def _print_percentiles(percentiles: dict[str, float]):
        """æ‰“å°åˆ†ä½ç‚¹ç»Ÿè®¡ï¼ˆè¾…åŠ©å‡½æ•°ï¼‰"""
        if not percentiles:
            print("    (æ— æ•°æ®)")
            return
        
        sorted_percentiles = sorted(percentiles.keys(), key=lambda x: int(x[1:]))
        
        # æ¯è¡Œæ˜¾ç¤º 5 ä¸ªåˆ†ä½ç‚¹ï¼Œè®©è¾“å‡ºæ›´ç´§å‡‘
        for i in range(0, len(sorted_percentiles), 5):
            line_items = []
            for percentile_key in sorted_percentiles[i:i+5]:
                percentile_value = percentiles[percentile_key]
                p_num = percentile_key[1:]  # å»æ‰ 'p' å‰ç¼€
                line_items.append(f"{p_num}%: {percentile_value:.0f}")
            print(f"    {' | '.join(line_items)}")
    
    def print_summary(self, stats: DatasetStats):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("HALUMEM DATASET STATISTICS")
        print("=" * 80 + "\n")
        
        print("ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»ç”¨æˆ·æ•°:              {stats.total_users}")
        print(f"  æ€» Session æ•°:         {stats.total_sessions}")
        print(f"  æ€»å¯¹è¯æ•°:              {stats.total_dialogues}")
        
        print(f"\nğŸ“ˆ å¹³å‡å€¼:")
        print(f"  æ¯ä¸ªç”¨æˆ·çš„å¹³å‡ Session æ•°:           {stats.avg_sessions_per_user:.2f}")
        print(f"  æ¯ä¸ª Session çš„å¹³å‡å¯¹è¯æ•°:           {stats.avg_dialogues_per_session:.2f}")
        print(f"  æ¯ä¸ª Session çš„å¹³å‡å¯¹è¯é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰: {stats.avg_dialogue_length_per_session:.2f}")
        
        print(f"\nğŸ“Š åˆ†å¸ƒç»Ÿè®¡:")
        if stats.sessions_per_user_list:
            print(f"  æ¯ç”¨æˆ· Session æ•° - æœ€å°: {min(stats.sessions_per_user_list)}, "
                  f"æœ€å¤§: {max(stats.sessions_per_user_list)}")
        
        if stats.dialogues_per_session_list:
            print(f"  æ¯ Session å¯¹è¯æ•° - æœ€å°: {min(stats.dialogues_per_session_list)}, "
                  f"æœ€å¤§: {max(stats.dialogues_per_session_list)}")
        
        if stats.dialogue_lengths_per_session_list:
            print(f"  æ¯ Session å¯¹è¯é•¿åº¦ - æœ€å°: {min(stats.dialogue_lengths_per_session_list)}, "
                  f"æœ€å¤§: {max(stats.dialogue_lengths_per_session_list)}")
        
        print(f"\nğŸ’¬ Content è¯¦ç»†ç»Ÿè®¡:")
        print(f"  æ€» Content æ•°é‡:       {stats.total_contents}")
        print(f"    User æ¶ˆæ¯æ•°:         {stats.total_user_contents}")
        print(f"    Assistant æ¶ˆæ¯æ•°:    {stats.total_assistant_contents}")
        print(f"  Content å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰:")
        print(f"    æœ€å°å€¼:              {stats.min_content_size}")
        print(f"    æœ€å¤§å€¼:              {stats.max_content_size}")
        
        if stats.content_sizes:
            avg_content_size = sum(stats.content_sizes) / len(stats.content_sizes)
            print(f"    å¹³å‡å€¼:              {avg_content_size:.2f}")
        
        print(f"\nğŸ“ˆ Content å¤§å°åˆ†ä½ç‚¹ (å…¨éƒ¨):")
        self._print_percentiles(stats.percentiles)
        
        print(f"\nğŸ“ˆ Content å¤§å°åˆ†ä½ç‚¹ (User è§’è‰²):")
        self._print_percentiles(stats.user_percentiles)
        
        print(f"\nğŸ“ˆ Content å¤§å°åˆ†ä½ç‚¹ (Assistant è§’è‰²):")
        self._print_percentiles(stats.assistant_percentiles)
        
        print(f"\nâœ‚ï¸  Session åˆ†å‰²ç»Ÿè®¡ (æŒ‰ 5000 å­—ç¬¦åˆ†å‰²):")
        print(f"  åŸå§‹ Session æ€»æ•°:      {stats.total_sessions}")
        print(f"  åˆ†å‰²å Chunk æ€»æ•°:      {stats.total_chunks_after_split}")
        print(f"  æ¯ä¸ªç”¨æˆ·å¹³å‡ Chunk æ•°:  {stats.avg_chunks_per_user:.2f}")
        print(f"  Chunk/Session æ¯”ä¾‹:     {stats.total_chunks_after_split / stats.total_sessions:.2f}x")
        
        print("\n" + "=" * 80)
    
    def print_per_user_stats(self):
        """æ‰“å°æ¯ä¸ªç”¨æˆ·çš„è¯¦ç»†ç»Ÿè®¡"""
        print("\n" + "=" * 80)
        print("PER-USER STATISTICS")
        print("=" * 80 + "\n")
        
        for idx, user_stats in enumerate(self.user_stats_list, 1):
            avg_dialogues = (
                sum(user_stats.dialogues_per_session) / len(user_stats.dialogues_per_session)
                if user_stats.dialogues_per_session else 0
            )
            avg_length = (
                sum(user_stats.dialogue_lengths_per_session) / len(user_stats.dialogue_lengths_per_session)
                if user_stats.dialogue_lengths_per_session else 0
            )
            
            print(f"[{idx}] {user_stats.user_name} (UUID: {user_stats.uuid[:8]}...)")
            print(f"    Session æ•°: {user_stats.num_sessions}")
            print(f"    åˆ†å‰²å Chunk æ•°: {user_stats.num_chunks_after_split}")
            print(f"    å¹³å‡æ¯ Session å¯¹è¯æ•°: {avg_dialogues:.2f}")
            print(f"    å¹³å‡æ¯ Session å¯¹è¯é•¿åº¦: {avg_length:.2f} å­—ç¬¦")
            print()
    
    def print_user_split_summary(self):
        """æ‰“å°æ¯ä¸ªç”¨æˆ·çš„åˆ†å‰²ç»Ÿè®¡æ‘˜è¦ï¼ˆè¡¨æ ¼å½¢å¼ï¼‰"""
        print("\n" + "=" * 80)
        print("PER-USER SESSION SPLIT SUMMARY (æŒ‰ 5000 å­—ç¬¦åˆ†å‰²)")
        print("=" * 80 + "\n")
        
        # è¡¨å¤´
        print(f"{'åºå·':<6} {'ç”¨æˆ·å':<25} {'åŸå§‹Sessions':<15} {'åˆ†å‰²åChunks':<15} {'æ¯”ä¾‹':<10}")
        print("-" * 80)
        
        # æ¯ä¸ªç”¨æˆ·çš„æ•°æ®
        for idx, user_stats in enumerate(self.user_stats_list, 1):
            ratio = (
                user_stats.num_chunks_after_split / user_stats.num_sessions
                if user_stats.num_sessions > 0 else 0
            )
            print(f"{idx:<6} {user_stats.user_name[:24]:<25} {user_stats.num_sessions:<15} "
                  f"{user_stats.num_chunks_after_split:<15} {ratio:.2f}x")
        
        print("-" * 80)
        
        # æ€»è®¡
        total_sessions = sum(u.num_sessions for u in self.user_stats_list)
        total_chunks = sum(u.num_chunks_after_split for u in self.user_stats_list)
        overall_ratio = total_chunks / total_sessions if total_sessions > 0 else 0
        
        print(f"{'æ€»è®¡':<6} {'':<25} {total_sessions:<15} {total_chunks:<15} {overall_ratio:.2f}x")
        print("=" * 80)
    
    def save_results(self, output_path: str, stats: DatasetStats):
        """ä¿å­˜ç»Ÿè®¡ç»“æœåˆ° JSON æ–‡ä»¶"""
        results = {
            "summary": {
                "total_users": stats.total_users,
                "total_sessions": stats.total_sessions,
                "total_dialogues": stats.total_dialogues,
                "avg_sessions_per_user": stats.avg_sessions_per_user,
                "avg_dialogues_per_session": stats.avg_dialogues_per_session,
                "avg_dialogue_length_per_session": stats.avg_dialogue_length_per_session,
                "session_split_stats": {
                    "total_chunks_after_split": stats.total_chunks_after_split,
                    "avg_chunks_per_user": stats.avg_chunks_per_user,
                    "chunk_to_session_ratio": (
                        stats.total_chunks_after_split / stats.total_sessions
                        if stats.total_sessions > 0 else 0
                    )
                },
                "content_stats": {
                    "total_contents": stats.total_contents,
                    "total_user_contents": stats.total_user_contents,
                    "total_assistant_contents": stats.total_assistant_contents,
                    "min_content_size": stats.min_content_size,
                    "max_content_size": stats.max_content_size,
                    "avg_content_size": (
                        sum(stats.content_sizes) / len(stats.content_sizes)
                        if stats.content_sizes else 0
                    ),
                    "percentiles_all": stats.percentiles,
                    "percentiles_user": stats.user_percentiles,
                    "percentiles_assistant": stats.assistant_percentiles
                }
            },
            "per_user_stats": [
                {
                    "user_name": u.user_name,
                    "uuid": u.uuid,
                    "num_sessions": u.num_sessions,
                    "num_chunks_after_split": u.num_chunks_after_split,
                    "avg_dialogues_per_session": (
                        sum(u.dialogues_per_session) / len(u.dialogues_per_session)
                        if u.dialogues_per_session else 0
                    ),
                    "avg_dialogue_length_per_session": (
                        sum(u.dialogue_lengths_per_session) / len(u.dialogue_lengths_per_session)
                        if u.dialogue_lengths_per_session else 0
                    ),
                    "dialogues_per_session": u.dialogues_per_session,
                    "dialogue_lengths_per_session": u.dialogue_lengths_per_session
                }
                for u in self.user_stats_list
            ]
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Results saved to: {output_path}")


def main(data_path: str, output_path: str = None, show_per_user: bool = False):
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(data_path).exists():
        logger.error(f"File not found: {data_path}")
        return
    
    # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
    analyzer = DatasetAnalyzer(data_path)
    analyzer.load_and_analyze()
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    stats = analyzer.compute_dataset_stats()
    
    # æ‰“å°æ‘˜è¦
    analyzer.print_summary(stats)
    
    # æ‰“å°æ¯ä¸ªç”¨æˆ·çš„åˆ†å‰²ç»Ÿè®¡æ‘˜è¦ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼‰
    analyzer.print_user_split_summary()
    
    # æ‰“å°æ¯ä¸ªç”¨æˆ·çš„è¯¦ç»†ç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰
    if show_per_user:
        analyzer.print_per_user_stats()
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    if output_path:
        analyzer.save_results(output_path, stats)
    else:
        # é»˜è®¤ä¿å­˜åˆ°ä¸æ•°æ®æ–‡ä»¶ç›¸åŒç›®å½•
        default_output = str(Path(data_path).parent / "dataset_statistics.json")
        analyzer.save_results(default_output, stats)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Analyze HaluMem dataset statistics"
    )
    parser.add_argument(
        "--data_path",
        type=str,
        required=True,
        help="Path to HaluMem JSONL file"
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default=None,
        help="Path to save statistics JSON (default: dataset_statistics.json in same dir)"
    )
    parser.add_argument(
        "--show_per_user",
        action="store_true",
        help="Show detailed statistics for each user"
    )
    
    args = parser.parse_args()
    
    main(
        data_path=args.data_path,
        output_path=args.output_path,
        show_per_user=args.show_per_user
    )
