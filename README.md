# Quick Start

## Hello Experience Maker
Here is a simple user guide for ExperienceMaker.

### Step0: Preparation Work

#### Prepare LLM & EMBEDDING_MODEL
We need to prepare the API services for the LLM and the Embedding model. 
Since we are using an OpenAI-compatible service, we only need to write the `OPENAI_API_KEY` and `OPENAI_BASE_URL` into the environment.
```shell
export OPENAI_API_KEY="sk-xxx"
export OPENAI_BASE_URL="xxx"
```

#### Prepare Vector Store
If you want to use vector store, you need to set up a vector database. Don't forget to set up the `ES_HOSTS`.
- Elasticsearch [quick start](../vector_store/elasticsearch.md)

#### Prepare Your Own Agent
Assume you have a runnable agent.
Here, we use a basic LLM combined with a simple react framework including three tools(code, web_search, terminate) as an example.
```python
class YourOwnAgent(...):
    ...
    def think(self, **kwargs) -> bool:
        ...

    def act(self, **kwargs):
        ...    

    def run(self, query: str, previous_experience: str):
        ...
```

Here is an [example code](./your_own_agent.py) with [prompt](./your_own_agent_prompt.yaml). To use this simple agent, you will need to set up `DASHSCOPE_API_KEY`.
```shell
export DASHSCOPE_API_KEY="sk-xxx"
```

### Step1: Start ExperienceMaker Http Service
- Install dependencies.
```shell
pip install .
```

- We start our context and summary services in `simple` mode. Next, you can use standard HTTP interfaces to call the services.
```shell
python -m experiencemaker.em_service \
 --port=8001 \
 --llm='{"backend": "openai_compatible", "model_name": "qwen3-32b", "temperature": 0.6}' \
 --embedding_model='{"backend": "openai_compatible", "model_name": "text-embedding-v4", "dimensions": 1024}' \
 --vector_store='{"backend": "elasticsearch"}' \
 --context_generator='{"backend": "simple"}' \
 --summarizer='{"backend": "simple"}'
```

- You can also use the more comprehensive StepSummarizer and StepContextGenerator, which provide more fine-grained functionality for processing trajectories (for more comprehensive documentation, see: cookbook/step_agent/readme.md)
```shell
python -m experiencemaker.em_service \
--port=8001 \
--llm='{"backend": "openai_compatible", "model_name": "qwen-max-2025-01-25", "temperature": 0.6}' \
--embedding_model='{"backend": "openai_compatible", "model_name": "text-embedding-v4", "dimensions": 1024}' \
--vector_store='{"backend": "elasticsearch"}' \
--agent_wrapper='{"backend": "simple"}' \
--context_generator='{"backend": "step", "enable_llm_rerank": true, "enable_context_rewrite": true, "enable_score_filter": false, "vector_retrieve_top_k": 15, "final_top_k": 5, "min_score_threshold": 0.3}' \
--summarizer='{"backend": "step", "enable_step_segmentation": false, "enable_similar_comparison": false, "enable_experience_validation": true, "max_retries": 3,"max_workers":16}'

# Parameter Description

## StepSummarizer
### enable_step_segmentation: Segment trajectories into meaningful step sequences
### enable_similar_comparison: Compare similar sequences between success/failure
### enable_experience_validation: Validate experience quality before storage

## StepContextGenerator
### enable_llm_rerank: Rerank retrieved experiences by relevance using LLM
### enable_context_rewrite: Rewrite context to be more task-specific
### enable_score_filter: Filter experiences by quality scores
```


### Step2: Enhance Your Own Agent

Call the capabilities of ContextGenerator and Summarizer through `EMClient`.

```python
from experiencemaker.em_client import EMClient

em_client = EMClient(base_url="http://0.0.0.0:8001")
```

Assume you have a list of messages generated by an agent; you can use the summarizer to generate experiences from them.

```python
request = SummarizerRequest(trajectories=[Trajectory(query=query, steps=messages, answer=messages[-1].content, done=True)], workspace_id="w_1234")
response = em_client.call_summarizer(request)
for experience in response.experiences:
    print(experience.model_dump_json())
```

Assume you have a query; you can use the context generator to generate a new query from the query and the related
experiences.

```python
request = ContextGeneratorRequest(trajectory=Trajectory(query=query), retrieve_top_k=1, workspace_id="w_1234")
response = em_client.call_context_generator(request)
new_query = f"{response.context_msg.content}\n\nUser Question\n{query}"
```

The complete code can be found in the implementation of [YourOwnAgentEnhanced](./your_own_agent_enhanced.py).
