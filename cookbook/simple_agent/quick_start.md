# Quick Start

## Hello Experience Maker
Here is a simple user guide for ExperienceMaker.

### Step0: Preparation Work

#### Prepare LLM & EMBEDDING_MODEL
We need to prepare the API services for the LLM and the Embedding model. 
Since we are using an OpenAI-compatible service, we only need to write the `OPENAI_API_KEY` and `OPENAI_BASE_URL` into the environment.
```shell
export OPENAI_API_KEY="sk-xxx"
export OPENAI_BASE_URL="xxx"
```

#### Prepare Vector Store
If you want to use vector store, you need to set up a vector database. Don't forget to set up the `ES_HOSTS`.
- Elasticsearch [quick start](../vector_store/elasticsearch.md)

#### Prepare Your Own Agent
Assume you have a runnable agent.
Here, we use a basic LLM combined with a simple react framework including three tools(code, web_search, terminate) as an example.
```python
class YourOwnAgent(...):
    ...
    def think(self, **kwargs) -> bool:
        ...

    def act(self, **kwargs):
        ...    

    def run(self, query: str, previous_experience: str):
        ...
```

Here is an [example code](./your_own_agent.py) with [prompt](./your_own_agent_prompt.yaml). To use this simple agent, you will need to set up `DASHSCOPE_API_KEY`.
```shell
export DASHSCOPE_API_KEY="sk-xxx"
```

### Step1: Start Experience Maker Http Service
- Install dependencies.
```shell
pip install .
```

- We start our context and summary services in `simple` mode. Next, you can use standard HTTP interfaces to call the services.
```shell
pip install .
python -m experiencemaker.em_service \
 --port=8001 \
 --llm='{"backend": "openai_compatible", "model_name": "qwen3-32b", "temperature": 0.6}' \
 --embedding_model='{"backend": "openai_compatible", "model_name": "text-embedding-v4", "dimensions": 1024}' \
 --vector_store='{"backend": "elasticsearch", "index_name": "your_own_agent"}' \
 --context_generator='{"backend": "simple", "retrieve_top_k": 1}' \
 --summarizer='{"backend": "simple"}'
```




### Step2: Implement AgentWrapper

In order to utilize the **context generator** and **summarizer** capabilities of experiencemaker, please inherit from **MxcAgent** and **BaseAgentWrapperMixin** to implement the AgentWrapper.

Here, you need to customize two parts:
- how to integrate the content message(insight) generated by the `self.context_generator` into the context.
- implement the execute function to output the trajectory.

Below is a simple example of integrating **trajectory-level insight** into the context.

```python
from experiencemaker.core.module.agent_wrapper.base_agent_wrapper import BaseAgentWrapperMixin

class MxcAgentWrapper(MxcAgent, BaseAgentWrapperMixin):
    def execute(self, query: str, **kwargs) -> Trajectory:
        trajectory = Trajectory(steps=messages, query=query)
        context_msg = self.context_generator.execute(trajectory=trajectory)
        new_query = f"""
previous insight:
{context_msg.content}
Please consider the helpful parts from these in answering the question, to make the response more comprehensive and substantial.

user query:
{query}
        """.strip()
        
        messages = self.run(new_query, **kwargs)
        return Trajectory(query=query, steps=messages, answer=messages[-1].content, done=True)

```

### Step3: Run AgentRunner with insight

Once you have completed the implementation of the AgentWrapper class, you will be able to utilize the capabilities of
experiencemaker. 

Here is an example using **SimpleAgentRunner**. 
We first executed two historical tasks, then summarized the experience and made it persistent. 
Finally, we utilized the historical experience in a new task.

[insights demo](./insight.json)


```python
from experiencemaker.core.module.runner.simple_agent_runner import SimpleAgentRunner



mxc_agent_wrapper = MxcAgentWrapper(llm=OpenAICompatibleBaseLLM(model_name="qwen3-32b", temperature=0.0001),
                                    max_steps=10,
                                    tools=[CodeTool(), DashscopeSearchTool(), TerminateTool()])
agent_runner = SimpleAgentRunner(agent_wrapper=mxc_agent_wrapper, summarizer="default", context_generator="default")

# historical tasks
agent_runner.rollout_trajectory(query="Analyze the company Tesla.")
agent_runner.rollout_trajectory(query="Analyze the company Apple.")

# summary insights and store them
agent_runner.summary_and_store()

# run agent with historical insights
trajectory = agent_runner.rollout_trajectory(query="Analyze the company Xiaomi Corporation.")
```


### Step4: Evaluation(Optional)

If we have a reward function that allows us to compare the performance before and after adding context, we can try this
part.

Use `run_agent` to obtain the answer from the original agent (answer1), and use `run_agent_wrapper` to get the answer
with added insights and experience (answer2).

Here, the reward function is used to compare and score the two answers. The `reward.reward_value` indicates the win rate
of answer2.

```python
# task
query = "Analyze Xiaomi Corporation."

# run agent
agent = MxcAgent(llm=OpenAICompatibleBaseLLM(model_name="qwen3-32b", temperature=0.0001),
                 max_steps=10,
                 tools=[CodeTool(), DashscopeSearchTool(), TerminateTool()])
messages = agent.run(query=query)
answer1 = messages[-1].content

# agent runner: Assume we already have some historical experience.
mxc_agent_wrapper = MxcAgentWrapper(llm=OpenAICompatibleBaseLLM(model_name="qwen3-32b", temperature=0.0001),
                                    max_steps=10,
                                    tools=[CodeTool(), DashscopeSearchTool(), TerminateTool()])
agent_runner = SimpleAgentRunner(agent_wrapper=mxc_agent_wrapper, summarizer="default", context_generator="default")
trajectory = agent_runner.rollout_trajectory(query=query)
answer2 = trajectory.answer

# pair-wise LLM evaluation
from experiencemaker.core.module.reward_fn.simple_reward_fn import SimpleRewardFn
reward_fn = SimpleRewardFn(llm=OpenAICompatibleBaseLLM(model_name="qwen3-32b", temperature=0.0001))
reward = reward_fn.execute(query=query, answer1=answer1, answer2=answer2, eval_times=5)
print(f"final reward={reward.reward_value}")
```