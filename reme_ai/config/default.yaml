# default config.yaml
backend: mcp
language: ""
thread_pool_max_workers: 32
ray_max_workers: 1

mcp:
  transport: sse
  host: "0.0.0.0"
  port: 8002

http:
  host: "0.0.0.0"
  port: 8002
  timeout_keep_alive: 600
  limit_concurrency: 64

flow:
  retrieve_task_memory_simple:
    flow_content: build_query_op >> recall_vector_store_op >> merge_memory_op
    description: "Retrieve the most relevant top_k memory experience from historical memory based on the query to help solve tasks better now"
    input_schema:
      query:
        type: "str"
        description: "current query"
        required: true

  summary_task_memory_simple:
    flow_content: simple_summary_op >> update_vector_store_op
    description: "Summarize trajectories or messages into memories"
    input_schema:
      trajectories:
        type: "list"
        description: "A list of conversation trajectory information, including message content and score. This field does not need to be filled in, the system will complete it automatically."
        required: false

  retrieve_task_memory:
    flow_content: build_query_op >> recall_vector_store_op >> rerank_memory_op >> rewrite_memory_op
    description: "Retrieve the most relevant top_k memory experience from historical memory based on the query to help solve tasks better now"
    input_schema:
      query:
        type: "str"
        description: "current query"
        required: true

  summary_task_memory:
    flow_content: trajectory_preprocess_op->[success_extraction_op|failure_extraction_op|comparative_extraction_op]->experience_validation_op->experience_deduplication_op->update_vector_store_op
    description: "Summarize trajectories or messages into memories"
    input_schema:
      trajectories:
        type: "list"
        description: "A list of conversation trajectory information, including message content and score. This field does not need to be filled in, the system will complete it automatically."
        required: false

#  vector_store: vector_store_action_op
#  agent: react_op

llm:
  default:
    backend: openai_compatible
    model_name: qwen3-30b-a3b-thinking-2507
    params:
      temperature: 0.6

embedding_model:
  default:
    backend: openai_compatible
    model_name: text-embedding-v4
    params:
      dimensions: 1024

vector_store:
  default:
    backend: elasticsearch
    embedding_model: default
    params:
      # hosts: "http://localhost:9200"
      hosts: "http://11.160.132.46:8200"
