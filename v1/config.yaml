# demo config.yaml

#python -m experiencemaker.em_service \
# --port=8001 \
# --llm='{"backend": "openai_compatible", "model_name": "qwen3-32b", "temperature": 0.6}' \
# --embedding_model='{"backend": "openai_compatible", "model_name": "text-embedding-v4", "dimensions": 1024}' \
# --vector_store='{"backend": "elasticsearch"}' \

http_service:
  host: "0.0.0.0"
  port: 8001
  timeout_keep_alive: 600
  limit_concurrency: 64

# -http_service.port=8001

thread_pool:
  max_workers: 20


api:
  step_retriever: mock1_op->mock2_op->mock3_op
  step_summarizer: mock1_op->[mock4_op->mock2_op|mock5_op]->mock3_op
  vector_store: mock6_op

# -api.step_retriever=mock1_op->[mock4_op->mock2_op|mock5_op]->mock3_op


# -op.mock1_op.a=1

op:
  mock1_op:
    backend: mock1_op
    a: 1
    b: 2
    llm: default
    vector_store: default
  mock2_op:
    backend: mock2_op
    a: 1
  mock3_op:
    backend: mock3_op
  mock4_op:
    backend: mock4_op
  mock5_op:
    backend: mock5_op
  mock6_op:
    backend: mock6_op

llm:
  default:
    backend: openai_compatible
    model_name: qwen3-32b
    temperature: 0.6

embedding_model:
  default:
    backend: openai_compatible
    model_name: text-embedding-v4
    dimensions: 1024

vector_store:
  default:
    backend: elasticsearch
    embedding_model: default
    hosts: "http://localhost:9200"

