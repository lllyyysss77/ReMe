# Context Compression Logic Comparison

## 旧逻辑 vs 新逻辑对比

### 旧逻辑流程（先计算总 token）

```
1. 接收所有消息
2. 计算所有消息的 token 总数
3. ❌ 问题：包含了系统消息、历史消息、最近消息的所有 token
4. 判断是否超过阈值
5. 如果超过，才分离系统消息和待压缩消息
6. 执行压缩
```

**问题**：
- 系统消息通常很大（例如：详细的 system prompt），会占用大量 token
- 即使历史消息不多，也可能因为系统消息触发压缩
- 最近消息也参与了阈值计算，但实际上不会被压缩

### 新逻辑流程（先分离，只计算待压缩部分）✅

```
1. 接收所有消息
2. 分离消息：
   - 系统消息（默认不压缩）
   - 待压缩消息（旧的历史消息）
   - 最近消息（不压缩）
3. ✅ 只计算待压缩消息的 token 数
4. 判断是否超过阈值
5. 如果超过，才执行压缩
6. 执行压缩
```

**优势**：
- ✅ 精确控制：只针对真正需要压缩的内容进行判断
- ✅ 避免误触发：系统消息不影响压缩决策
- ✅ 更合理：最近消息本来就要保留，不应参与阈值计算

## 实际案例对比

### 案例 1：大型系统消息场景

**场景**：
- 系统消息：5000 tokens（包含详细的角色设定、规则、示例等）
- 历史消息（7条）：4000 tokens
- 最近消息（3条）：2000 tokens
- **阈值设置**：10000 tokens

#### 旧逻辑：
```
总 token = 5000 + 4000 + 2000 = 11000 tokens
11000 > 10000 ✓ 触发压缩
压缩 7 条历史消息（4000 tokens）
```
**结果**：虽然历史消息只有 4000 tokens，但因为系统消息很大，还是触发了压缩。

#### 新逻辑：
```
系统消息：5000 tokens - 保留（不参与计算）
待压缩消息：4000 tokens - 检查阈值
4000 < 10000 ✗ 不触发压缩
最近消息：2000 tokens - 保留
```
**结果**：历史消息只有 4000 tokens，不需要压缩，节省了 LLM API 调用成本！

---

### 案例 2：大量历史消息场景

**场景**：
- 系统消息：1000 tokens
- 历史消息（50条）：25000 tokens
- 最近消息（3条）：2000 tokens
- **阈值设置**：10000 tokens

#### 旧逻辑：
```
总 token = 1000 + 25000 + 2000 = 28000 tokens
28000 > 10000 ✓ 触发压缩
压缩 50 条历史消息（25000 tokens）
```

#### 新逻辑：
```
系统消息：1000 tokens - 保留
待压缩消息：25000 tokens - 检查阈值
25000 > 10000 ✓ 触发压缩
最近消息：2000 tokens - 保留
压缩 50 条历史消息（25000 tokens）
```
**结果**：两种逻辑都正确触发压缩，新逻辑计算更精确。

---

### 案例 3：边界情况

**场景**：
- 系统消息：8000 tokens
- 历史消息（5条）：3000 tokens
- 最近消息（3条）：1000 tokens
- **阈值设置**：10000 tokens

#### 旧逻辑：
```
总 token = 8000 + 3000 + 1000 = 12000 tokens
12000 > 10000 ✓ 触发压缩
压缩 5 条历史消息（3000 tokens）
```
**问题**：只有 3000 tokens 的历史消息被压缩，压缩效果有限，但要调用 LLM。

#### 新逻辑：
```
系统消息：8000 tokens - 保留
待压缩消息：3000 tokens - 检查阈值
3000 < 10000 ✗ 不触发压缩
最近消息：1000 tokens - 保留
```
**结果**：避免了不必要的压缩，节省成本！

## 总结

| 指标 | 旧逻辑 | 新逻辑 |
|------|--------|--------|
| **计算对象** | 所有消息 | 仅待压缩消息 |
| **系统消息影响** | ❌ 会影响 | ✅ 不影响 |
| **最近消息影响** | ❌ 会影响 | ✅ 不影响 |
| **压缩精确性** | ⚠️ 可能误触发 | ✅ 精确控制 |
| **成本控制** | ⚠️ 可能浪费 API 调用 | ✅ 更节省成本 |
| **适用场景** | 简单场景 | 所有场景 |

## 关键改进点

1. **🎯 精确的阈值控制**：只针对真正需要压缩的内容
2. **💰 成本优化**：避免不必要的 LLM API 调用
3. **🔧 逻辑合理性**：系统消息和最近消息本就不压缩，不应影响决策
4. **📊 更好的可预测性**：用户可以更准确地预测何时触发压缩

